"""
Local-only CLI RAG chatbot using an Ollama-compatible API.
- Generates SQL text without executing it.
- Uses local embeddings + chat models served by Ollama (or compatible) at OLLAMA_BASE_URL.
"""

import argparse
import json
import os
from datetime import datetime
from typing import List, Tuple

import httpx
import numpy as np
from dotenv import load_dotenv

load_dotenv()

RAG_INDEX_PATH = os.getenv("RAG_INDEX_PATH", "rag_index.json")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_EMBED_MODEL = os.getenv("OLLAMA_EMBED_MODEL", "nomic-embed-text")
OLLAMA_CHAT_MODEL = os.getenv("OLLAMA_CHAT_MODEL", "llama3")
OLLAMA_TIMEOUT = float(os.getenv("OLLAMA_TIMEOUT", "60"))


def load_rag_index(path: str) -> List[dict]:
    if not os.path.exists(path):
        raise FileNotFoundError(f"RAG index not found: {path}")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def cosine(a, b) -> float:
    a = np.array(a, dtype=float)
    b = np.array(b, dtype=float)
    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-8
    return float(np.dot(a, b) / denom)


def get_embedding(text: str) -> List[float]:
    url = f"{OLLAMA_BASE_URL}/api/embeddings"
    payload = {"model": OLLAMA_EMBED_MODEL, "prompt": text}
    with httpx.Client(timeout=OLLAMA_TIMEOUT) as client:
        resp = client.post(url, json=payload)
        resp.raise_for_status()
        data = resp.json()
    return data.get("embedding", [])


def rag_retrieve(question: str, rag_index: List[dict], top_k: int = 3) -> List[dict]:
    q_emb = get_embedding(question)

    scored = []
    for item in rag_index:
        score = cosine(q_emb, item.get("embedding", []))
        scored.append((score, item))

    scored.sort(key=lambda x: x[0], reverse=True)
    return [s[1] for s in scored[:top_k]]


def format_context_blocks(contexts: List[dict]) -> Tuple[str, List[dict]]:
    ctx_blocks = []
    tables = []

    for context in contexts:
        ctx_blocks.append(context.get("text", ""))

        table_name = context.get("table_name", "")
        columns = ""
        for line in context.get("text", "").split("\n"):
            if line.startswith("Columns:"):
                columns = line.replace("Columns:", "").strip()
                break

        tables.append({"table": table_name, "columns": columns})

    return "\n\n---\n\n".join(ctx_blocks), tables


def generate_sql(question: str, rag_index: List[dict]) -> Tuple[str, List[dict]]:
    contexts = rag_retrieve(question, rag_index, top_k=3)
    ctx_text, tables = format_context_blocks(contexts)

    prompt = f"""
You are an expert MySQL assistant for the game Mafia42.

You are given context about the database schema.
Use ONLY the tables and columns that are consistent with this context.
Do NOT invent any new table or column.
Write a single valid MySQL SELECT query. No comments, no markdown, no explanation.

[CONTEXT]
{ctx_text}

[QUESTION]
{question}

[OUTPUT RULES]
- Output ONLY raw SQL.
- Do NOT wrap in ```sql ``` or any code fences.
- Use proper table and column names based on the context.
"""

    url = f"{OLLAMA_BASE_URL}/api/chat"
    payload = {
        "model": OLLAMA_CHAT_MODEL,
        "messages": [
            {"role": "system", "content": "You are an expert MySQL assistant."},
            {"role": "user", "content": prompt},
        ],
        "stream": False,
    }

    with httpx.Client(timeout=OLLAMA_TIMEOUT) as client:
        resp = client.post(url, json=payload)
        resp.raise_for_status()
        data = resp.json()

    # Ollama returns a list of messages when stream=False
    message = data.get("message", {})
    content = message.get("content", "").strip()
    sql = content.replace("```sql", "").replace("```", "").replace("`", "").strip()
    return sql, tables


def main() -> None:
    parser = argparse.ArgumentParser(description="Local (Ollama) RAG-based SQL generator (no execution).")
    parser.add_argument(
        "--rag-index",
        default=RAG_INDEX_PATH,
        help="Path to rag_index.json generated by build_rag_index.py",
    )
    args = parser.parse_args()

    try:
        rag_index = load_rag_index(args.rag_index)
    except FileNotFoundError as exc:
        print(f"âŒ {exc}")
        return

    print("ğŸ’¬ ë¡œì»¬ RAG ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. 'exit' ë˜ëŠ” 'quit'ìœ¼ë¡œ ì¢…ë£Œí•˜ì„¸ìš”.")
    print(
        f"ğŸ“š ìŠ¤í‚¤ë§ˆ ì—”íŠ¸ë¦¬: {len(rag_index)}ê°œ (chat_model={OLLAMA_CHAT_MODEL}, embed_model={OLLAMA_EMBED_MODEL})"
    )
    print(f"ğŸ”Œ ëŒ€ìƒ ì—”ë“œí¬ì¸íŠ¸: {OLLAMA_BASE_URL}")

    while True:
        question = input("ì§ˆë¬¸> ").strip()
        if not question:
            continue
        if question.lower() in {"exit", "quit"}:
            break

        timestamp = datetime.now().isoformat(timespec="seconds")
        print(f"\n[{timestamp}] SQL ìƒì„± ì¤‘...\n")

        try:
            sql, tables = generate_sql(question, rag_index)
        except Exception as exc:
            print(f"âŒ SQL ìƒì„± ì‹¤íŒ¨: {exc}")
            continue

        print("--- ì œì•ˆëœ SQL ------------------------------")
        print(sql)
        print("-------------------------------------------")

        if tables:
            used = ", ".join([t["table"] for t in tables if t.get("table")])
            if used:
                print(f"ğŸ“Š ì‚¬ìš©ëœ í…Œì´ë¸”: {used}")
        print()


if __name__ == "__main__":
    main()
